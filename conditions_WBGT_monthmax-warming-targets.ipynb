{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4ff7a9-222d-4d32-b1be-1fb093ed2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac9bf4-9c43-4cfe-aff7-9475063cd305",
   "metadata": {},
   "source": [
    "# conditions_WBGT_monthmax-warming-targets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca7a29-42aa-483f-a1c1-0249b536a6a2",
   "metadata": {},
   "source": [
    "'''\n",
    "    File name: conditions_WBGT_monthmax.ipynb\n",
    "    Author: Andreas Prein\n",
    "    E-mail: prein@ucar.edu\n",
    "    Date created: 04.30.2024\n",
    "    Date last modified: 04.30.2024\n",
    "\n",
    "    ############################################################## \n",
    "    Needs data from:\n",
    "    papers/2024/2024_WBGT_Climate-Change/programs/process_conditions_WBGT_monthmax/process_conditions_WBGT_monthmax.ipynb\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "    1) Read in preprocessed data\n",
    "    2) Analyze the dependence between changes in WBGT and its source variables\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9533d4-aaa1-4f57-985b-5c201a143db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import rrule\n",
    "import datetime\n",
    "import glob\n",
    "from netCDF4 import Dataset\n",
    "import sys, traceback\n",
    "import dateutil.parser as dparser\n",
    "import string\n",
    "from pdb import set_trace as stop\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "# import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "import pylab as plt\n",
    "import random\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy\n",
    "import shapefile\n",
    "import matplotlib.path as mplPath\n",
    "from matplotlib.patches import Polygon as Polygon2\n",
    "# Cluster specific modules\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import seaborn as sns\n",
    "# import metpy.calc as mpcalc\n",
    "import shapefile as shp\n",
    "import sys\n",
    "from scipy.signal import wiener, filtfilt, butter, gaussian, freqz\n",
    "from scipy.ndimage import filters\n",
    "import pickle\n",
    "import time\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddffb0d-97a6-4119-b3d0-3cd14ff031e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 0 #int(sys.argv[1])\n",
    "cy = city\n",
    "warming_tar = 2 #int(sys.argv[2]) #2\n",
    "\n",
    "cities = ['Phoenix',\n",
    "          'Miami',\n",
    "          'New Orleans',\n",
    "          'Houston',\n",
    "          'Atlanta',\n",
    "          'DC',\n",
    "          # 'Yuma',\n",
    "          'Sacramento',\n",
    "          'Dalas',\n",
    "          'Charlotte',\n",
    "          'Chicago',\n",
    "          'New York City',\n",
    "          'Tampa',\n",
    "            ]\n",
    "city_loc = [\n",
    "            [[350,370],[370,390]],\n",
    "            [[1120,1135],[170,225]],\n",
    "            [[860,880],[255,265]],\n",
    "            [[730,760],[240,270]],\n",
    "            [[980,1000],[373,400]],\n",
    "            [[1110,1135],[540,575]],\n",
    "            # [[292,300],[365,370]],\n",
    "            [[180,195],[550,570]],\n",
    "            [[685,720],[325,350]],\n",
    "            [[1055,1070],[430,450]],\n",
    "            [[875,900],[780,810]],\n",
    "            [[1160,1180],[600,635]],\n",
    "            [[1055,1070],[230,240]],\n",
    "           ]\n",
    "\n",
    "temp_channge = np.arange(0,3.1,0.1)\n",
    "delta_t = 0.25\n",
    "minyears = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd4a31b-3dff-4e5a-a65c-52b95cff9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in constant fields\n",
    "# read CONUS404 coordinates\n",
    "ncid=Dataset('/glade/u/home/prein/projects/2020/2020_CONUS404/data/wrfconstants_d01_CONUS404.nc4', mode='r')\n",
    "lon_conus = np.array(np.squeeze(ncid.variables['XLONG'][:]))\n",
    "lat_conus = np.array(np.squeeze(ncid.variables['XLAT'][:]))\n",
    "ncid.close()\n",
    "\n",
    "from wrf import (to_np, getvar, smooth2d, get_cartopy, cartopy_xlim,\n",
    "                 cartopy_ylim, latlon_coords)\n",
    "import cartopy.feature as cf\n",
    "import cartopy.crs as ccrs\n",
    "GEO_EM_D1 = '/glade/u/home/prein/projects/2020/2020_CONUS404/data/wrfconstants_d01_CONUS404.nc4'\n",
    "ncfile = Dataset(GEO_EM_D1)\n",
    "HGT_M = getvar(ncfile, \"HGT\")\n",
    "LU = getvar(ncfile, \"LU_INDEX\")\n",
    "cart_proj = get_cartopy(HGT_M)\n",
    "ncid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d13d9a-c19e-40ab-933b-99da945bc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth's surface.\n",
    "\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and Longitude of point 1 (in degrees)\n",
    "        lat2, lon2: Latitude and Longitude of point 2 (in degrees)\n",
    "\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Earth radius in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "def calculate_distance_grid(lat1, lon1, lat_grid, lon_grid):\n",
    "    \"\"\"\n",
    "    Calculate the distance between a single point (lat1, lon1) and a grid of points.\n",
    "\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and Longitude of the single point (in degrees)\n",
    "        lat_grid, lon_grid: 2D arrays of Latitude and Longitude of the grid points (in degrees)\n",
    "\n",
    "    Returns:\n",
    "        2D array of distances in kilometers.\n",
    "    \"\"\"\n",
    "    # Ensure the inputs are numpy arrays\n",
    "    lat_grid = np.array(lat_grid)\n",
    "    lon_grid = np.array(lon_grid)\n",
    "\n",
    "    # Use broadcasting to calculate distances\n",
    "    distances = haversine_distance(lat1, lon1, lat_grid, lon_grid)\n",
    "\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722b898b-701e-4a3e-aa25-ba3b84a14aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is the center of the city?\n",
    "lat_cent = int((city_loc[cy][1][0]+city_loc[cy][1][1])/2)\n",
    "lon_cent = int((city_loc[cy][0][0]+city_loc[cy][0][1])/2)\n",
    "dist = haversine_distance(lat_conus[lat_cent,lon_cent], lon_conus[lat_cent,lon_cent],\n",
    "                 lat_conus, lon_conus)\n",
    "radius = 50 # km\n",
    "urban_cells = (dist <= radius) &  (LU == 13)\n",
    "rural_cells = (dist <= radius) &  (LU != 13) & (LU != 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813ba5d1-4c08-4095-9a7f-eef4da68d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices that have to read from the grid\n",
    "def find_cluster_bounding_box(grid):\n",
    "    \"\"\"\n",
    "    Find the bounding box of a cluster of ones in a 2D array.\n",
    "    \n",
    "    Parameters:\n",
    "        grid (2D array): A 2D numpy array with zeros and a cluster of ones.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (start_lat, start_lon, end_lat, end_lon)\n",
    "               Grid indices of the bounding box containing the ones.\n",
    "    \"\"\"\n",
    "    # Find the indices of the ones\n",
    "    ones_indices = np.argwhere(grid == 1)\n",
    "    \n",
    "    if ones_indices.size == 0:\n",
    "        raise ValueError(\"The grid does not contain any ones.\")\n",
    "    \n",
    "    # Determine the bounds\n",
    "    start_lat, start_lon = ones_indices.min(axis=0)  # Top-left corner\n",
    "    end_lat, end_lon = ones_indices.max(axis=0)  # Bottom-right corner\n",
    "    \n",
    "    return start_lat, start_lon, end_lat, end_lon\n",
    "\n",
    "lat0, lon0, lat1, lon1 = find_cluster_bounding_box(dist <= radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3f8e03-7153-41de-8459-0896d784a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to read everithing\n",
    "lat0 =  0\n",
    "lon0 =  0\n",
    "lat1 =  lon_conus.shape[0]\n",
    "lon1 =  lon_conus.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecb9d8f-6ae5-408a-ac39-e6376a952cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_cells = urban_cells[lat0:lat1,lon0:lon1]\n",
    "rural_cells = rural_cells[lat0:lat1,lon0:lon1]\n",
    "lon_conus = lon_conus[lat0:lat1,lon0:lon1]\n",
    "lat_conus = lat_conus[lat0:lat1,lon0:lon1]\n",
    "HGT_M = HGT_M[lat0:lat1,lon0:lon1]\n",
    "LU = LU[lat0:lat1,lon0:lon1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047551b-c937-40a9-b465-f8d960ed6373",
   "metadata": {},
   "source": [
    "### WBGT calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812669e4-1d4a-4eba-86f3-1a6f460224d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /glade/u/home/prein/MyPython_Programs/python/packages/cdo-1.2.1/lib/python2.7/site-packages/cdo-1.2.1-py2.7.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cftime in /glade/work/prein/conda-envs/python_202401/lib/python3.11/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy>1.13.3 in /glade/work/prein/conda-envs/python_202401/lib/python3.11/site-packages (from cftime) (1.25.2)\n",
      "\u001b[33mDEPRECATION: Loading egg at /glade/u/home/prein/MyPython_Programs/python/packages/cdo-1.2.1/lib/python2.7/site-packages/cdo-1.2.1-py2.7.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numba in /glade/work/prein/conda-envs/python_202401/lib/python3.11/site-packages (0.58.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /glade/work/prein/conda-envs/python_202401/lib/python3.11/site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in /glade/work/prein/conda-envs/python_202401/lib/python3.11/site-packages (from numba) (1.25.2)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libcudart.so.11.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcalendar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monthrange\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# import functions for calculating cosine zenith angle\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoszenith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coszda, cosza\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# import functions for calculating WBGT\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mWBGT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WBGT_Liljegren, WBGT_GCM\n",
      "\u001b[0;31mImportError\u001b[0m: libcudart.so.11.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Create Python function for wbgt calculation\n",
    "!pip install cftime\n",
    "!pip install numba\n",
    "# import general packages\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import njit, vectorize\n",
    "# import packages needed for obtaining google cloud data\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "import sys \n",
    "from calendar import monthrange\n",
    "# import functions for calculating cosine zenith angle\n",
    "from coszenith import coszda, cosza\n",
    "# import functions for calculating WBGT\n",
    "from WBGT import WBGT_Liljegren, WBGT_GCM\n",
    "\n",
    "def wbgt(huss,\n",
    "        tas,\n",
    "        ps,\n",
    "        uas,\n",
    "        vas,\n",
    "        rlds,\n",
    "        rsds,\n",
    "        rlus,\n",
    "        rsus,\n",
    "        lat,\n",
    "        lon,\n",
    "        time):\n",
    "\n",
    "    \n",
    "\n",
    "    # function that creates the correct xarray data\n",
    "    def xr_data(data,\n",
    "            time_hh,\n",
    "            lon,\n",
    "            lat):\n",
    "            \n",
    "\n",
    "        lat = xr.DataArray(lat,\n",
    "                               dims = ['x_logical', 'y_logical'])\n",
    "        \n",
    "        lon = xr.DataArray(lon,\n",
    "                               dims = ['x_logical', 'y_logical'])\n",
    "        \n",
    "        \n",
    "        new_array = xr.DataArray(data, dims=['time','x_logical','y_logical'])\n",
    "        new_array.coords['lat'] = lat\n",
    "        new_array.coords['lon'] = lon\n",
    "        new_array.coords['time'] = time_hh\n",
    "        new_array.groupby('lat').mean()\n",
    "        \n",
    "        ds = xr.Dataset({'testdata': (('time','y', 'x'), data)},\n",
    "                         coords={'time': (('time'), time_hh),\n",
    "                                 'lat': (('y', 'x'), lat.data),\n",
    "                                 'lon': (('y', 'x'), lon.data)})\n",
    "        \n",
    "        data = xr.DataArray(ds.to_array()[0,:], dims=ds.dims, coords=ds.coords)\n",
    "    \n",
    "        return data\n",
    "\n",
    "    # bring the input data into the correct format\n",
    "    huss = xr_data(huss, time, lon, lat)\n",
    "    tas = xr_data(tas, time, lon, lat)\n",
    "    ps = xr_data(ps, time, lon, lat)\n",
    "    uas = xr_data(uas, time, lon, lat)\n",
    "    vas = xr_data(vas, time, lon, lat)\n",
    "    rlds = xr_data(rlds, time, lon, lat)\n",
    "    rsds = xr_data(rsds, time, lon, lat)\n",
    "    rlus = xr_data(rlus, time, lon, lat)\n",
    "    rsus = xr_data(rsus, time, lon, lat)\n",
    "            \n",
    "    # not nescessary with WRF interpolate radiation\n",
    "    rsdsinterp=rsds\n",
    "    rsusinterp=rsus\n",
    "    rldsinterp=rlds\n",
    "    rlusinterp=rlus\n",
    "    # rsdsdiffinterp=interp(rsdsdiff)\n",
    "    \n",
    "    \n",
    "    # In[12]:\n",
    "    \n",
    "    \n",
    "    # # check that radiation fields have correct time coordinates\n",
    "    # rsdsinterp.time\n",
    "    \n",
    "    \n",
    "    # ### 3.2 calculate $\\cos\\theta$\n",
    "    # We calculate $\\cos\\theta$ averaged both during each interval and during only the sunlit period of each interval. Both of them are needed for calculating WBGT. For the details of the calculation procedure of $\\cos\\theta$ please refer to Hogan and Hirahara (2016), and Di Napoli et al (2020)\n",
    "    \n",
    "    # In[13]:\n",
    "    \n",
    "    \n",
    "    # create meshgrid of latitude and longitude, and we will calculate cosine zenith angle on these grids\n",
    "    # lon,lat=np.meshgrid(huss.lon,huss.lat)\n",
    "    \n",
    "    lat=lat*np.pi/180\n",
    "    lon=lon*np.pi/180\n",
    "    \n",
    "    \n",
    "    # calculate $\\cos\\theta$ averaged during each interval.\n",
    "    # \n",
    "    # the function ***cosza*** asks for four inputs: \n",
    "    # - a time series (UTC time) indicating the center points of intervals for which the $\\cos\\theta$ will be calculated\n",
    "    # - the latitude and longitude arrays across which $\\cos\\theta$ will be calculated\n",
    "    # - the length of each interval (e.g. 3 for 3hourly)\n",
    "    \n",
    "    # In[92]:\n",
    "    \n",
    "    \n",
    "    # specifiy the time seris for which we want to calculate cosine zenith angle \n",
    "    date=xr.DataArray(huss.time.values,dims=('time'),coords={'time':huss.time}).chunk({'time':8})\n",
    "    # use dask.array map_blocks to calculate cosine zenith angle lazily and parallelly\n",
    "    cza=da.map_blocks(cosza,date.data,lat,lon,3,chunks=(np.min([huss.shape[0], 8]),lat.shape[0],lat.shape[1]),new_axis=[1,2])\n",
    "    # transfer to xarray DataArray\n",
    "    cza=np.array(xr.DataArray(cza,dims=huss.dims, coords=huss.coords))\n",
    "    \n",
    "    # specifiy the time seris for which we want to calculate cosine zenith angle \n",
    "    date=xr.DataArray(huss.time.values,dims=('time'),coords={'time':huss.time}).chunk({'time':8})\n",
    "    # use dask.array map_blocks to calculate cosine zenith angle lazily and parallelly\n",
    "    cza=da.map_blocks(cosza,date.data,lat,lon,3,chunks=(np.min([huss.shape[0], 8]),lat.shape[0],lat.shape[1]),new_axis=[1,2])\n",
    "    # transfer to xarray DataArray\n",
    "    cza=np.array(xr.DataArray(cza,dims=huss.dims, coords=huss.coords))\n",
    "    \n",
    "    # calculate $\\cos\\theta$ averaged during only the sunlit part of each interval.\n",
    "    # \n",
    "    # the function ***coszda*** asks for four inputs: \n",
    "    # - a time series (UTC time) indicating the center points of intervals for which the $\\cos\\theta$ will be calculated\n",
    "    # - the latitude and longitude arrays across which $\\cos\\theta$ will be calculated\n",
    "    # - the length of each interval (e.g. 3 for 3hourly)\n",
    "    \n",
    "    \n",
    "    \n",
    "    czda=da.map_blocks(coszda,date.data,lat,lon,3,chunks=(np.min([huss.shape[0], 8]),lat.shape[0],lat.shape[1]),new_axis=[1,2])\n",
    "    # transfer to xarray DataArray\n",
    "    czda=xr.DataArray(czda,dims=huss.dims, coords=huss.coords)\n",
    "    # here we change zero (czda=0 when the sun is below horizon) to an arbitrary negative value (here we choose -0.5) to avoid division by zero \n",
    "    czda=xr.where(czda<=0,-0.5,czda)\n",
    "    \n",
    "    \n",
    "    # Have a look at the $\\cos\\theta$ for the first time step\n",
    "    \n",
    "    # # In[16]:\n",
    "    \n",
    "    \n",
    "    # cza[0,:,:].plot()\n",
    "    \n",
    "    \n",
    "    # # In[17]:\n",
    "    \n",
    "    \n",
    "    # czda[0,:,:].plot()\n",
    "    \n",
    "    \n",
    "    # ### 3.3 Calculate relative humidity\n",
    "    # Here we calculate relative humidity from specific humidity since relative humidity is one of the required inputs for WBGT function.\n",
    "    \n",
    "    \n",
    "    # calculate saturated vapor pressure\n",
    "    @vectorize\n",
    "    def esat(tas,ps):\n",
    "        # tas: temperature (K)\n",
    "        # ps: surface pressure (Pa)\n",
    "        # return saturation vapor pressure (Pa)\n",
    "        if tas>273.15:\n",
    "            es = 611.21 * np.exp(17.502 * (tas - 273.15) *((tas - 32.18)**(-1)))\n",
    "            es = (1.0007 + (3.46*10**(-6) * ps/100)) * es\n",
    "        else:\n",
    "            es = 611.15 * np.exp(22.452 * (tas - 273.15) * ((tas - 0.6)**(-1)))\n",
    "            es=(1.0003 + (4.18*10**(-6) * ps/100)) * es\n",
    "        return es\n",
    "    # calculate vapor pressure\n",
    "    def vaporpres(huss, ps):\n",
    "        # huss: specific humidity (kg/kg)\n",
    "        # ps: surface pressure (Pa)\n",
    "        # return vapor pressure (Pa)\n",
    "        r=huss*((1-huss)**(-1))\n",
    "        return ps*r*((0.622+r)**(-1))\n",
    "    # calculate relative humidity from specific humidity\n",
    "    def huss2rh(tas,huss,ps):\n",
    "        # tas: temperature (K)\n",
    "        # huss: specific humidity (kg/kg)\n",
    "        # ps: surface pressure (Pa)\n",
    "        # return relative humidity (%)\n",
    "        return vaporpres(huss, ps)*(esat(tas,ps)**(-1))*100\n",
    "    \n",
    "    \n",
    "    # In[19]:\n",
    "    \n",
    "    \n",
    "    # calculate relative humidity\n",
    "    hurs=xr.apply_ufunc(huss2rh,tas,huss,ps) #,dask=\"parallelized\",output_dtypes=[float])\n",
    "    # set the maximum value as 100%\n",
    "    hurs=xr.where(hurs>100,100,hurs)\n",
    "    \n",
    "    \n",
    "    # ### 3.4 Calculate wind speed\n",
    "    # Calculate wind speed from the u and v components of wind. Keep in mind that the wind speed are output at 10m height. Our WBGT function will ask for a boolean argument to indicate whether 2m (```True```) or 10m (```False```) wind is used.\n",
    "    \n",
    "    # In[38]:\n",
    "    \n",
    "    \n",
    "    func = lambda x, y: np.sqrt(x ** 2 + y ** 2)\n",
    "    # sfcwind=xr.apply_ufunc(func, uas, vas) #,dask=\"parallelized\",output_dtypes=[float])\n",
    "    sfcwind = np.sqrt(uas ** 2 + vas ** 2)\n",
    "    \n",
    "    \n",
    "    # ### 3.5 Calculate the ratio of direct solar radiation\n",
    "    # We need to know the ratio of direct solar radiation in order to get the radiation heat gain right for WBGT calculation, since the treatment of direct and diffuse solar radiation are different in the code. \n",
    "    # \n",
    "    # In this case, we have ```rsdsdiff``` which is the diffuse solar radiation, so we can directly calculate the ratio of direct solar radiation. \n",
    "    # \n",
    "    # When we don't have such a field, we need to call the **```fdir```** function (also can be imported from the ```WBGT``` module) to calculate the ratio. Please refer to the ```WBGT.pyx``` file for how to call **```fdir```** function.\n",
    "    \n",
    "    # In[39]:\n",
    "    \n",
    "    \n",
    "    from WBGT import fdir, fdir_3d\n",
    "    # In[40]:\n",
    "    \n",
    "    f=fdir_3d(np.atleast_3d(cza),np.atleast_3d(czda),np.atleast_3d(rsds),date)\n",
    "    \n",
    "    \n",
    "    # In[41]:\n",
    "    \n",
    "    \n",
    "    # f=(rsdsinterp-rsdsdiffinterp)/rsdsinterp \n",
    "    # the treatments below aim to avoid unrealistically high or low values of f which are also included in Liljegren's code.\n",
    "    f=xr.where(cza<=np.cos(89.5/180*np.pi),0,f) \n",
    "    f=xr.where(f>0.9,0.9,f)\n",
    "    f=xr.where(f<0,0,f)\n",
    "    f=xr.where(rsdsinterp<=0,0,f)\n",
    "    \n",
    "    \n",
    "    # # 4. Calculate WBGT\n",
    "    \n",
    "    # ### 4.1 Liljegren's original formulation\n",
    "    # In Liljegren's original formulation, downward solar radiation is the only required radiation input, other radiation components are approximated internally.\n",
    "    # \n",
    "    # The meaning of each argument:\n",
    "    # - ```tas```: air temperature (K)\n",
    "    # -  ```hurs```: relative humidity (%)\n",
    "    # -  ```sfcwind```: 2 meter wind speed (m/s)\n",
    "    # -  ```ps```: surface pressure (Pa)\n",
    "    # -  ```rsdsinterp```: surface downward solar radiation (w/m2)\n",
    "    # -  ```f```: the ratio of direct solar radiation \n",
    "    # - ```czda```: average cosine zenith angle during only the sunlit period of each interval\n",
    "    # \n",
    "    # -  the ```False``` argument at the end tell the function that our wind speed is not at 2meter height which will make  the function to treat wind speed as 10 meter height and transfer it to 2 meter. The function currently only support 2 meter and 10meter wind. For wind speed at other heights, users need to change the source code slightly.\n",
    "    \n",
    "    # In[45]:\n",
    "    \n",
    "    \n",
    "    wbgt_liljegren=xr.apply_ufunc(WBGT_Liljegren,tas,hurs,ps,sfcwind,rsdsinterp,f,czda,False,dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # We also exposed the functions for calculating natural wet bulb temperature and black globe temperature, two important components of WBGT\n",
    "    \n",
    "    # In[46]:\n",
    "    \n",
    "    \n",
    "    from WBGT import Tg_Liljegren, Tnwb_Liljegren\n",
    "    \n",
    "    \n",
    "    # In[48]:\n",
    "    \n",
    "    \n",
    "    # calculate black globe temperature\n",
    "    tg_liljegren=xr.apply_ufunc(Tg_Liljegren,tas,hurs,ps,sfcwind,rsdsinterp,f,czda,False,dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # In[49]:\n",
    "    \n",
    "    \n",
    "    # calculate natural wet bulb temperature\n",
    "    tnwb_liljegren=xr.apply_ufunc(Tnwb_Liljegren,tas,hurs,ps,sfcwind,rsdsinterp,f,czda,False,dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # Let's plot calculated WBGT at the last time step\n",
    "    \n",
    "    # In[ ]:\n",
    "    \n",
    "    \n",
    "    # plt.pcolormesh(wbgt_liljegren[-1,:,:]-273, cmap='coolwarm')\n",
    "    \n",
    "    \n",
    "    # ### 4.2 Our modified formulation\n",
    "    # We modify Liljegren's original code to take advantage of the full set of radiation components available from climate model output. \n",
    "    # \n",
    "    # The meaning of each argument:\n",
    "    # - ```tas```: air temperature (K)\n",
    "    # -  ```hurs```: relative humidity (%)\n",
    "    # -  ```sfcwind```: 2 meter wind speed (m/s)\n",
    "    # -  ```ps```: surface pressure (Pa)\n",
    "    # -  ```rsdsinterp```: surface downward solar radiation (w/m2)\n",
    "    # -  ```rsusinterp```: surface reflected solar radiation (w/m2)\n",
    "    # -  ```rldsinterp```: surface downward long-wave radiation (w/m2)\n",
    "    # -  ```rlusinterp```: surface upwelling long-wave radiation (w/m2)\n",
    "    # - ```f```: the ratio of direct solar radiation \n",
    "    # -  ```czda```: average cosine zenith angle during only the sunlit period of each interval\n",
    "    # - ```False```: indicate we are using 10m wind\n",
    "    \n",
    "    # In[51]:\n",
    "    \n",
    "    \n",
    "    # calculate WBGT\n",
    "    wbgt_gcm=xr.apply_ufunc(WBGT_GCM,tas,hurs,ps,sfcwind,rsdsinterp,rsusinterp,rldsinterp,rlusinterp,f,czda,False,dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # calculate components of WBGT\n",
    "    \n",
    "    # from WBGT import Tg_GCM, Tnwb_GCM\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # calculate black globe temperature\n",
    "    # tg_gcm=xr.apply_ufunc(Tg_GCM,tas,ps,sfcwind,rsdsinterp,rsusinterp,rldsinterp,rlusinterp,f,czda,False, dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # # calculate natural wet bulb temperature\n",
    "    # tnwb_gcm=xr.apply_ufunc(Tnwb_GCM,tas,hurs,ps,sfcwind,rsdsinterp,rsusinterp,rldsinterp,rlusinterp,f,czda,False,dask=\"parallelized\",output_dtypes=[float])\n",
    "    \n",
    "    \n",
    "    # Let's plot monthly average WBGT.\n",
    "    \n",
    "    \n",
    "    # # wbgt_gcm.mean('time').plot()\n",
    "    # plt.pcolormesh(wbgt_gcm[-1,:,:]-273, cmap='coolwarm', vmin=-40,vmax=40)\n",
    "    \n",
    "    ### Save data to netcdf\n",
    "    return wbgt_gcm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a111ac-4ed1-49f2-9d33-74d99f02ff03",
   "metadata": {},
   "source": [
    "### User imput section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89c6039-8264-4938-8d8d-9fdd96bae37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/glade/campaign/mmm/c3we/prein/Papers/2024/2024_WBGT-climate-change/WBGT_monmax_variables/'\n",
    "savedir = '/glade/campaign/mmm/c3we/prein/Papers/2024/2024_WBGT-climate-change/city_changes/'\n",
    "\n",
    "time_c404_ctr = pd.date_range(datetime.datetime(1980, 1, 1, 0), \n",
    "                              end=datetime.datetime(2019, 12, 31, 23), freq='h')\n",
    "years_ctr = np.unique(time_c404_ctr.year)\n",
    "conus404_pgw_dir = '/glade/campaign/mmm/c3we/prein/CONUS404/data/MonthlyData_PGW/'\n",
    "time_c404_pgw = pd.date_range(datetime.datetime(1980, 1, 1, 0), \n",
    "                              end=datetime.datetime(2019, 12, 31, 23), freq='h')\n",
    "years_pgw = np.unique(time_c404_pgw.year)\n",
    "\n",
    "variables = ['GWBT','T2','Q2','PSFC','U10','V10','LWDNB','SWDNB','LWUPB','SWUPB']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047f156-a309-481e-836d-8fa91afedccf",
   "metadata": {},
   "source": [
    "### Load global average LENS2 temperature for warming level selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b0b364-b268-46ab-abfe-9e2c5e1f8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/glade/u/home/prein/papers/2024/2024_WBGT_Climate-Change/data/NASA_global-av_t2m/graph.txt', delimiter=r\"\\s+\", header=0, skiprows=[0,1,2,4])\n",
    "\n",
    "StartDay = datetime.datetime(1850, 1, 15,1)\n",
    "StopDay = datetime.datetime(2101, 1, 15,0)\n",
    "Time=pd.date_range(StartDay, end=StopDay, freq='M')\n",
    "Time_years=pd.date_range(StartDay, end=StopDay, freq='Y')\n",
    "years = np.unique(Time.year)\n",
    "\n",
    "reference_period = [1950, 1980]\n",
    "\n",
    "lens2_t2m_dir = '/glade/campaign/mmm/c3we/mingge/DOD/LENS2/'\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMNAV_monthly_globalAvg_185001-197912.nc', mode='r')\n",
    "TREFMNAV_hist = np.array(np.squeeze(ncid.variables['TREFMNAV'][:]))\n",
    "ncid.close()\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMNAV_monthly_globalAvg_198001-201412.nc', mode='r')\n",
    "TREFMNAV_cur = np.array(np.squeeze(ncid.variables['TREFMNAV'][:]))\n",
    "ncid.close()\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMNAV_monthly_globalAvg_201501-210012.nc', mode='r')\n",
    "TREFMNAV_fut = np.array(np.squeeze(ncid.variables['TREFMNAV'][:]))\n",
    "ncid.close()\n",
    "\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMXAV_monthly_globalAvg_185001-197912.nc', mode='r')\n",
    "TREFMXAV_hist = np.array(np.squeeze(ncid.variables['TREFMXAV'][:]))\n",
    "ncid.close()\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMXAV_monthly_globalAvg_198001-201412.nc', mode='r')\n",
    "TREFMXAV_cur = np.array(np.squeeze(ncid.variables['TREFMXAV'][:]))\n",
    "ncid.close()\n",
    "ncid=Dataset(lens2_t2m_dir + 'TREFMXAV_monthly_globalAvg_201501-210012.nc', mode='r')\n",
    "TREFMXAV_fut = np.array(np.squeeze(ncid.variables['TREFMXAV'][:]))\n",
    "ncid.close()\n",
    "\n",
    "TREFMNAV = np.append(TREFMNAV_hist, TREFMNAV_cur, axis=1)\n",
    "TREFMNAV = np.append(TREFMNAV, TREFMNAV_fut, axis=1)\n",
    "TREFMXAV = np.append(TREFMXAV_hist, TREFMXAV_cur, axis=1)\n",
    "TREFMXAV = np.append(TREFMXAV, TREFMXAV_fut, axis=1)\n",
    "lens2_t2m = (TREFMNAV + TREFMXAV) / 2.\n",
    "\n",
    "lens2_t2m_y = np.mean(np.reshape(lens2_t2m, (lens2_t2m.shape[0], int(lens2_t2m.shape[1]/12), 12)), axis=2)\n",
    "\n",
    "ref_t2m = np.mean(lens2_t2m_y[:,(years >= reference_period[0]) & (years <= reference_period[1])])\n",
    "time_to_t2m = np.mean(lens2_t2m_y, axis=0) - ref_t2m\n",
    "\n",
    "dc_hist = np.arange(0,1.5,0.1)\n",
    "dc_pgw = np.arange(0,3.5,0.1)\n",
    "dc = 0.25 # deg. C\n",
    "\n",
    "ref_period = [1950,1979]\n",
    "\n",
    "era5_ref = np.mean(data['No_Smoothing'][(data['Year'] >= ref_period[0]) & (data['Year'] <= ref_period[1])])\n",
    "era5_warming = np.squeeze(np.array([data['No_Smoothing'][data['Year'] == yy] - era5_ref for yy in range(1980,2020,1)]))\n",
    "\n",
    "lens2_ref = np.mean(time_to_t2m[(years >= ref_period[0]) & (years <= ref_period[1])])\n",
    "lens2_warming = np.array([time_to_t2m[years == yy][0] - lens2_ref for yy in range(2022,2022+len(np.unique(time_c404_pgw.year)),1)])\n",
    "\n",
    "pgw_warming = np.zeros((len(years_ctr))); pgw_warming[:] = np.nan\n",
    "for yy in range(len(years_ctr)):\n",
    "    lens_hist = (years <= years_ctr[yy] + 5) & (years >= years_ctr[yy] - 5)\n",
    "    lens_fut = (years <= years_ctr[yy] + 40 + 5) & (years >= years_ctr[yy] + 40 - 5)\n",
    "    pgw_warming[yy] = data['No_Smoothing'][years_ctr[yy] == data['Year']] + np.mean(time_to_t2m[lens_fut]) - np.mean(time_to_t2m[lens_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8257b37d-8735-40b2-bf78-9940060565fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_years_ref = (era5_warming >= warming_tar - dc) & (era5_warming <= warming_tar + dc)\n",
    "\n",
    "targ_years = (pgw_warming[:len(years_pgw)] >= warming_tar - dc) & (pgw_warming[:len(years_pgw)] <= warming_tar + dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9486f-dd2e-444b-ba7d-a70a398b38d2",
   "metadata": {},
   "source": [
    "# Read CONUS404 CTR OR PGW WBGT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca012d63-b8cc-478a-ac89-2a87efe3ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_data = np.zeros((len(years_ctr),12,lon_conus.shape[0],lon_conus.shape[1],len(variables))); ctr_data[:] = np.nan\n",
    "ctr_time = np.empty((len(years_ctr),12,lon_conus.shape[0],lon_conus.shape[1]), dtype='datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054f4ef2-e9ba-4db6-8cfe-0147adab1ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:22<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": [
    "for yy in tqdm(range(len(years_ctr))):\n",
    "    for mm in range(12):\n",
    "        datafile = data_dir + str(years_ctr[yy]) + str(mm+1).zfill(2) +'_WBGT_monmax_variables_ctr.npz'\n",
    "        if os.path.isfile(datafile) == True:\n",
    "            data = np.load(datafile)\n",
    "            ctr_data[yy,mm,:,:,:] = data['conus_ctr'][lat0:lat1,lon0:lon1,:]\n",
    "            ctr_time[yy,mm,:] = data['timestamp_max'][lat0:lat1,lon0:lon1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c30879b-4dd9-48f3-a997-a681268d0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgw_data = np.zeros((len(years_pgw),12,lon_conus.shape[0],lon_conus.shape[1],len(variables))); pgw_data[:] = np.nan\n",
    "pgw_time = np.empty((len(years_pgw),12,lon_conus.shape[0],lon_conus.shape[1]), dtype='datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37fa735e-4110-49b7-bac6-14260945ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:25<00:00,  5.14s/it]\n"
     ]
    }
   ],
   "source": [
    "for yy in tqdm(range(len(years_pgw))):\n",
    "    for mm in range(12):\n",
    "        datafile = data_dir + str(years_pgw[yy]) + str(mm+1).zfill(2) +'_WBGT_monmax_variables_pgw.npz'\n",
    "        if os.path.isfile(datafile) == True:\n",
    "            data = np.load(datafile)\n",
    "            pgw_data[yy,mm,:,:,:] = data['conus_ctr'][lat0:lat1,lon0:lon1,:]\n",
    "            pgw_time[yy,mm,:] = data['timestamp_max'][lat0:lat1,lon0:lon1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "450865bd-7d8c-48bb-83b2-7e07997f2aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "month_max = np.nanargmax(ctr_data[:,:,:,:,0], axis=1)\n",
    "y,la,lo = np.indices(ctr_data[:,0,:,:,0].shape)\n",
    "\n",
    "ymax_ctr = np.copy(ctr_data[:,0,:,:,:]); ymax_ctr[:]=np.nan\n",
    "for va in tqdm(range(len(variables))):\n",
    "    ymax_ctr[:,:,:,va] = ctr_data[y, month_max, la, lo, va]\n",
    "ytime_ctr = ctr_time[y, month_max, la, lo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "157503dc-3e3e-42c9-bda6-6654f2b9e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "month_max_pgw = np.nanargmax(pgw_data[:,:,:,:,0], axis=1)\n",
    "y,la,lo = np.indices(pgw_data[:,0,:,:,0].shape)\n",
    "\n",
    "ymax_pgw = np.copy(pgw_data[:,0,:,:,:]); ymax_pgw[:]=np.nan\n",
    "for va in tqdm(range(len(variables))):\n",
    "    ymax_pgw[:,:,:,va] = pgw_data[y, month_max_pgw, la, lo, va]\n",
    "ytime_pgw = pgw_time[y, month_max_pgw, la, lo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e2b299e-aa82-4918-8f12-ae06f3f5a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warming_tar_ref = 0.25\n",
    "targ_years_ref = (era5_warming >= warming_tar_ref - dc) & (era5_warming <= warming_tar_ref + dc)\n",
    "warming_tar = 2\n",
    "targ_years = (pgw_warming[:len(years_pgw)] >= warming_tar - dc) & (pgw_warming[:len(years_pgw)] <= warming_tar + dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75f141cc-68e9-4516-9695-4b7f66322068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to npz file\n",
    "data_ctr = np.transpose(ymax_ctr, (0, 3, 1, 2))\n",
    "data_pgw = np.transpose(ymax_pgw, (0, 3, 1, 2))\n",
    "\n",
    "# Create NetCDF file\n",
    "outfile = '/glade/campaign/mmm/c3we/prein/Papers/2024/2024_WBGT-climate-change/yearMax-WBGT_variabes.npz'\n",
    "\n",
    "np.savez(outfile,\n",
    "        data_ctr = data_ctr[targ_years_ref,:],\n",
    "        data_pgw = data_pgw[targ_years,:],\n",
    "        ytime_ctr = ytime_ctr[targ_years_ref,:],\n",
    "        ytime_pgw = ytime_pgw[targ_years,:],\n",
    "        lat_conus = lat_conus,\n",
    "        lon_conus = lon_conus,\n",
    "        variables = variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998287a2-2002-4292-af79-e491cab91d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.nanmean(np.nanmax(pgw_data[:,:,:,:,0], axis=(1)), axis=(1,2))-273.15, c='r')\n",
    "plt.plot(np.nanmean(np.nanmax(ctr_data[:,:,:,:,0], axis=(1)), axis=(1,2))-273.15, c='b')\n",
    "\n",
    "plt.show()\n",
    "# plt.plot(c_pgw_years, np.nanmean(conus_pgw_wbgt, axis=(1,2))-273.15, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4b812-d34e-499d-8343-b3dbe8b682bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fut = np.nanmean(np.nanmax(pgw_data[:,:,:,:,0], axis=(1)), axis=(1,2))-273.15\n",
    "cur = np.nanmean(np.nanmax(ctr_data[:,:,:,:,0], axis=(1)), axis=(1,2))-273.15\n",
    "plt.plot(fut - cur[:len(fut)], c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488436bf-d211-4bfa-b3a2-d9e2ae28aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vars = [['T2'],\n",
    "               ['Q2'],\n",
    "               ['PSFC'],\n",
    "               ['U10','V10'],\n",
    "               ['LWDNB','LWUPB'],\n",
    "               ['SWDNB','SWUPB']]\n",
    "\n",
    "change_var_categories = ['T2','Q2','PSFC','UV10', 'LW', 'SW']\n",
    "change_cat_colors = ['#e31a1c','#1f78b4','#969696','#33a02c','#a6cee3','#fb9a99']\n",
    "\n",
    "# select warming targets\n",
    "warming_tar_ref = 0.25\n",
    "targ_years_ref = (era5_warming >= warming_tar_ref - dc) & (era5_warming <= warming_tar_ref + dc)\n",
    "\n",
    "if warming_tar >1:\n",
    "    targ_years = (pgw_warming[:len(years_pgw)] >= warming_tar - dc) & (pgw_warming[:len(years_pgw)] <= warming_tar + dc)\n",
    "else:\n",
    "    targ_years = (era5_warming >= warming_tar - dc) & (era5_warming <= warming_tar + dc)\n",
    "\n",
    "city_cells = 50\n",
    "rural_city = [\"city\",\"rural\"]\n",
    "ctr_original_data = np.zeros((ymax_ctr[targ_years_ref].shape[0], city_cells, 2)); ctr_original_data[:] = np.nan\n",
    "if warming_tar > 1:\n",
    "    change_sensitivities = np.zeros((ymax_pgw[targ_years].shape[0], city_cells, len(change_vars),2)); change_sensitivities[:] = np.nan\n",
    "    pgw_original_data = np.zeros((ymax_pgw[targ_years].shape[0], city_cells,2)); pgw_original_data[:] = np.nan\n",
    "else:\n",
    "    change_sensitivities = np.zeros((ymax_ctr[targ_years].shape[0], city_cells, len(change_vars),2)); change_sensitivities[:] = np.nan\n",
    "    pgw_original_data = np.zeros((ymax_ctr[targ_years].shape[0], city_cells,2)); pgw_original_data[:] = np.nan\n",
    "\n",
    "print(cities[cy])\n",
    "savefile =  savedir + cities[cy] + '_' + str(city_cells) + 'gc_statistics_urban-rural_'+str(warming_tar)+'C*.npz'\n",
    "if os.path.isfile(savefile) == False:\n",
    "    for cr in range(len(rural_city)):\n",
    "        if rural_city[cr] == \"city\":\n",
    "            gc_sel = urban_cells\n",
    "        elif rural_city[cr] == \"city\":\n",
    "            gc_sel = rural_cells\n",
    "        # city_gc = LU[city_loc[cy][1][0]:city_loc[cy][1][1],\n",
    "        #              city_loc[cy][0][0]:city_loc[cy][0][1]] != 13 # 13 is urban\n",
    "        ctr_data = ymax_ctr[targ_years_ref,:]\n",
    "        ctr_data = np.copy(ctr_data[:,gc_sel])\n",
    "        \n",
    "        if warming_tar > 1:\n",
    "            warmer_data = ymax_pgw[targ_years,:]\n",
    "            warmer_data = np.copy(warmer_data[:,gc_sel])\n",
    "        else:\n",
    "            warmer_data = ymax_ctr[targ_years,:]\n",
    "            warmer_data = np.copy(warmer_data[:,gc_sel])\n",
    "        \n",
    "        lat_sel = lat_conus[gc_sel]\n",
    "        lon_sel = lon_conus[gc_sel]\n",
    "        \n",
    "        cell_sel = np.array([random.randint(0, ctr_data.shape[1]-1) for ii in range(np.min([city_cells, warmer_data.shape[1]]))])\n",
    "        for ce in tqdm(range(np.min([city_cells, warmer_data.shape[1]-1]))):\n",
    "            if ce == 49:\n",
    "                continue\n",
    "            try:\n",
    "                ctr_data_sel = np.copy(ctr_data[:,cell_sel[ce],:])\n",
    "            except:\n",
    "                stop()\n",
    "            warmer_data_sel = np.copy(warmer_data[:,cell_sel[ce],:])\n",
    "    \n",
    "            # calculate unperturbed WBGT (is necessary because values are not exactly the same)\n",
    "            ctr_original_data[:,ce,cr] = ctr_data_sel[:,variables.index('GWBT')]\n",
    "            pgw_original_data[:,ce,cr] = warmer_data_sel[:,variables.index('GWBT')]\n",
    "            # print('full change signal orig: '+ str(np.nanmean(warmer_data_sel[:,variables.index('GWBT')]) - np.nanmean(ctr_data_sel[:,variables.index('GWBT')])))\n",
    "            \n",
    "            for yy in range(ctr_data.shape[0]):\n",
    "                time_ctr_sel = ytime_ctr[targ_years_ref,:][yy, :][gc_sel]\n",
    "                ctr_original_data[yy,ce,cr] = wbgt(ctr_data_sel[yy,variables.index('Q2')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('T2')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('PSFC')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('U10')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('V10')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('LWDNB')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('SWDNB')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('LWUPB')][None,None,None],\n",
    "                                        ctr_data_sel[yy,variables.index('SWUPB')][None,None,None],\n",
    "                                        lat_sel[cell_sel[ce]][None,None],\n",
    "                                        lon_sel[cell_sel[ce]][None,None],\n",
    "                                        np.array([time_ctr_sel[cell_sel[ce]]]))\n",
    "            for yy in range(warmer_data.shape[0]):\n",
    "                if warming_tar > 1:\n",
    "                    time_pgw_sel = ytime_pgw[targ_years,:][yy, :][gc_sel]\n",
    "                else:\n",
    "                    time_pgw_sel = ytime_ctr[targ_years,:][yy, :][gc_sel]\n",
    "                pgw_original_data[yy,ce,cr] = wbgt(warmer_data_sel[yy,variables.index('Q2')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('T2')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('PSFC')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('U10')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('V10')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('LWDNB')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('SWDNB')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('LWUPB')][None,None,None],\n",
    "                                        warmer_data_sel[yy,variables.index('SWUPB')][None,None,None],\n",
    "                                        lat_sel[cell_sel[ce]][None,None],\n",
    "                                        lon_sel[cell_sel[ce]][None,None],\n",
    "                                        np.array([time_pgw_sel[cell_sel[ce]]]))\n",
    "            \n",
    "    \n",
    "            # print('full change signal calc: '+ str(np.nanmean(pgw_original_data[:,cy,ce]) - np.nanmean(ctr_original_data[:,cy,ce])))\n",
    "            \n",
    "            for va in range(len(change_vars)):\n",
    "                warmer_data_noTchange = np.copy(warmer_data_sel)\n",
    "                for sv in range(len(change_vars[va])):\n",
    "                    ivar = variables.index(change_vars[va][sv])\n",
    "                    warmer_data_noTchange[:,ivar] = warmer_data_noTchange[:,ivar] - \\\n",
    "                        (np.nanmean(warmer_data_sel[:,ivar]) - np.nanmean(ctr_data_sel[:,ivar]))\n",
    "    \n",
    "                # print(-(np.nanmean(warmer_data_sel[:,ivar]) - np.nanmean(ctr_data_sel[:,ivar])))\n",
    "                for yy in range(warmer_data.shape[0]):\n",
    "                    if warming_tar > 1:\n",
    "                        time_pgw_sel = ytime_pgw[targ_years,:][yy, :][gc_sel]\n",
    "                    else:\n",
    "                        time_pgw_sel = ytime_ctr[targ_years,:][yy, :][gc_sel]\n",
    "                    change_sensitivities[yy,ce,va,cr] = wbgt(warmer_data_noTchange[yy,variables.index('Q2')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('T2')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('PSFC')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('U10')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('V10')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('LWDNB')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('SWDNB')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('LWUPB')][None,None,None],\n",
    "                                        warmer_data_noTchange[yy,variables.index('SWUPB')][None,None,None],\n",
    "                                        lat_sel[cell_sel[ce]][None,None],\n",
    "                                        lon_sel[cell_sel[ce]][None,None],\n",
    "                                        np.array([time_pgw_sel[cell_sel[ce]]]))\n",
    "    \n",
    "                # print(change_var_categories[va]+' '+str(np.mean(pgw_original_data[:,cy,ce] - change_sensitivities[:,cy,ce,va])))\n",
    "                \n",
    "                \n",
    "                # if np.mean(pgw_original_data[:,cy,ce] - change_sensitivities[:,cy,ce,va]) < 0:\n",
    "                #     stop()\n",
    "    stop()\n",
    "    np.savez(savefile,\n",
    "            change_sensitivities_city = change_sensitivities[:,:,:],\n",
    "            ctr_original_data_city = ctr_original_data[:,:],\n",
    "            pgw_original_data_city = pgw_original_data[:,:],\n",
    "            rural_city = rural_city)\n",
    "else:\n",
    "    # load the data\n",
    "    data_cy = np.load(savefile)\n",
    "    change_sensitivities[:,:,:] = data_cy['change_sensitivities_city']\n",
    "    ctr_original_data[:,:] = data_cy['ctr_original_data_city']\n",
    "    pgw_original_data[:,:] = data_cy['pgw_original_data_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0c1db-f55f-448f-8a1d-4560c25109dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f71f90-f616-4fcb-920e-cf00c93ea364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrf import (to_np, getvar, smooth2d, get_cartopy, cartopy_xlim,\n",
    "                 cartopy_ylim, latlon_coords)\n",
    "import cartopy.feature as cf\n",
    "import cartopy.crs as ccrs\n",
    "GEO_EM_D1 = '/glade/u/home/prein/projects/2020/2020_CONUS404/data/wrfconstants_d01_CONUS404.nc4'\n",
    "ncfile = Dataset(GEO_EM_D1)\n",
    "HGT_M = getvar(ncfile, \"HGT\")\n",
    "LU = getvar(ncfile, \"LU_INDEX\")\n",
    "cart_proj = get_cartopy(HGT_M)\n",
    "ncid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe1079-d763-4d0b-8305-bbb92a30fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "mpl.rcParams['hatch.linewidth'] = 0.1  # previous pdf hatch linewidth\n",
    "\n",
    "gc=5\n",
    "def coarsen(data,\n",
    "            factor):\n",
    "    data = np.reshape(data[:int(int(data.shape[0]/factor)*factor),\n",
    "                           :int(int(data.shape[1]/factor)*factor)], \n",
    "                             (int(data.shape[0]/factor),\n",
    "                             factor,\n",
    "                             int(data.shape[1]/factor),\n",
    "                             factor))\n",
    "    return np.mean(data, axis = (1,3))\n",
    "\n",
    "def coarsen3d(data,\n",
    "            factor):\n",
    "    data = np.reshape(data[:,:int(int(data.shape[1]/factor)*factor),\n",
    "                             :int(int(data.shape[2]/factor)*factor)], \n",
    "                             (data.shape[0],\n",
    "                              int(data.shape[1]/factor),\n",
    "                             factor,\n",
    "                             int(data.shape[2]/factor),\n",
    "                             factor))\n",
    "    return np.mean(data, axis = (2,4))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "plt.rcParams.update({\"font.size\": 12})\n",
    "gs1 = gridspec.GridSpec(3, 4)\n",
    "gs1.update(left=0.04, right=0.96, bottom=0.08, top=0.95, wspace=0.01, hspace=0.35)\n",
    "from string import ascii_lowercase\n",
    "rgsLableABC = list(ascii_lowercase)\n",
    "\n",
    "XX = [0,1,2,3,0,1,2,3,0,1,2,3]\n",
    "YY = [0,0,0,0,1,1,1,1,2,2,2,2]\n",
    "\n",
    "# ['GWBT', 'T2', 'Q2', 'PSFC', 'U10', 'V10', 'LWDNB', 'SWDNB', 'LWUPB', 'SWUPB']\n",
    "unit = ['K','K','g kg$^{-1}$','Pa','m s$^{-1}$','m s$^{-1}$','W m$^{-2}$','W m$^{-2}$','W m$^{-2}$','W m$^{-2}$']\n",
    "var_ranges = [2.5,3.5, 3,200,1,1,35,35,35,35]\n",
    "for va in tqdm(range(len(variables))):\n",
    "    if warming_tar > 1:\n",
    "        data_pgw = np.mean(ymax_pgw[targ_years,:,:,va], axis=0)\n",
    "        sig = scipy.stats.mannwhitneyu(coarsen3d(ymax_pgw[targ_years,:,:,va], gc), \n",
    "                             coarsen3d(ymax_ctr[targ_years_ref,:,:,va], gc), axis = 0)\n",
    "    else:\n",
    "        data_pgw = np.mean(ymax_ctr[targ_years,:,:,va], axis=0)\n",
    "        sig = scipy.stats.mannwhitneyu(coarsen3d(ymax_ctr[targ_years,:,:,va], gc), \n",
    "                             coarsen3d(ymax_ctr[targ_years_ref,:,:,va], gc), axis = 0)\n",
    "    data_ctr = np.mean(ymax_ctr[targ_years_ref,:,:,va], axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    if variables[va] == 'Q2':\n",
    "        data_pgw = data_pgw*1000\n",
    "        data_ctr = data_ctr*1000\n",
    "        cmap = 'coolwarm_r'\n",
    "    else:\n",
    "        cmap = 'coolwarm'\n",
    "    \n",
    "    ax = plt.subplot(gs1[YY[va], XX[va]], projection=cart_proj)\n",
    "    \n",
    "    # plt.pcolormesh(rgrLon_c, rgrLat_c, hail_freq,\n",
    "    #               cmap=hail_prob_cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    plt.pcolormesh(coarsen(lon_conus, gc), coarsen(lat_conus, gc),\n",
    "                   coarsen(data_pgw, gc) - coarsen(data_ctr, gc), \n",
    "                   vmin=-var_ranges[va],vmax=var_ranges[va], cmap=cmap,\n",
    "                   transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add coastlines and borders\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=0.5)\n",
    "    # Add US states\n",
    "    ax.add_feature(cfeature.STATES, linestyle='-', edgecolor='black', alpha=0.5)\n",
    "    \n",
    "    # # Set extent to North America\n",
    "    # ax.set_extent([-127, -66, 24, 50], crs=ccrs.PlateCarree())\n",
    "    \n",
    "    # # Add gridlines\n",
    "    # gl = ax.gridlines(draw_labels=True)\n",
    "    # gl.bottom_labels=False\n",
    "    # gl.right_labels=False\n",
    "    \n",
    "    # Title\n",
    "    if variables[va] == 'GWBT':\n",
    "        plt.title(rgsLableABC[va] +') WBGT change')\n",
    "    else:\n",
    "        plt.title(rgsLableABC[va] +') '+ variables[va]+' change')\n",
    "\n",
    "    # add significance layer\n",
    "    ax.contourf(\n",
    "        coarsen(lon_conus, gc), coarsen(lat_conus, gc), \n",
    "        sig.pvalue < 0.05,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        colors='none',\n",
    "        levels=[.5,1.5],\n",
    "        hatches=['...',''],\n",
    "    )\n",
    "\n",
    "    # Add the colorbar\n",
    "    pos = ax.get_position()\n",
    "    # Calculate the new position for the colorbar\n",
    "    cbar_width = pos.width * 0.8  # 80% of the axis width\n",
    "    cbar_x = pos.x0 + (pos.width - cbar_width) / 2  # Centered horizontally\n",
    "    cbar_y = pos.y0 - 0.025  # Slightly below the axis\n",
    "    cbar_height = 0.01  # Height of the colorbar\n",
    "    \n",
    "    # Create a new axis for the colorbar\n",
    "    cbar_ax = fig.add_axes([cbar_x, cbar_y, cbar_width, cbar_height])\n",
    "\n",
    "    colorbar = plt.colorbar(label='change in '+variables[va] + ' [' + unit[va] + ']', \n",
    "                 orientation='horizontal', extend = 'both', cax=cbar_ax)\n",
    "    \n",
    "    # plt.contourf(coarsen(lon_conus, gc), coarsen(lat_conus, gc),\n",
    "    #              sig.pvalue, levels=[0.05,1], hatches=[\"\", \".\"], alpha=0,\n",
    "    #              transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# plot the median peak month of WBGT\n",
    "ax = plt.subplot(gs1[YY[va+1], XX[va+1]], projection=cart_proj)\n",
    "# plt.pcolormesh(rgrLon_c, rgrLat_c, hail_freq,\n",
    "#               cmap=hail_prob_cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "plt.pcolormesh(lon_conus, lat_conus, \n",
    "               np.median(month_max[targ_years_ref,:], axis=0), cmap=\"Paired\",vmin=0,vmax=11,\n",
    "               transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add coastlines and borders\n",
    "ax.coastlines()\n",
    "ax.add_feature(cartopy.feature.BORDERS, linestyle='-', alpha=0.5)\n",
    "# Add US states\n",
    "ax.add_feature(cfeature.STATES, linestyle='-', edgecolor='black', alpha=0.5)\n",
    "\n",
    "# # Set extent to North America\n",
    "# ax.set_extent([-127, -66, 24, 50], crs=ccrs.PlateCarree())\n",
    "\n",
    "# # Add gridlines\n",
    "# gl = ax.gridlines(draw_labels=True)\n",
    "# gl.bottom_labels=False\n",
    "# gl.right_labels=False\n",
    "\n",
    "# Title\n",
    "plt.title(rgsLableABC[va+1] +') WBGT peak month in ref. period')\n",
    "\n",
    "# Add the colorbar\n",
    "pos = ax.get_position()\n",
    "# Calculate the new position for the colorbar\n",
    "cbar_width = pos.width * 0.8  # 80% of the axis width\n",
    "cbar_x = pos.x0 + (pos.width - cbar_width) / 2  # Centered horizontally\n",
    "cbar_y = pos.y0 - 0.025  # Slightly below the axis\n",
    "cbar_height = 0.01  # Height of the colorbar\n",
    "\n",
    "# Create a new axis for the colorbar\n",
    "cbar_ax = fig.add_axes([cbar_x, cbar_y, cbar_width, cbar_height])\n",
    "\n",
    "colorbar = plt.colorbar(label='WBGT peak month', \n",
    "             orientation='horizontal', cax=cbar_ax)# , extend = 'both'\n",
    "tick_locations = np.linspace(0.5,10.5,12)\n",
    "tick_labels = [\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\"J\",\"A\",\"S\",\"O\",\"N\",\"D\"]\n",
    "colorbar.set_ticks(tick_locations)\n",
    "colorbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "sPlotFile=\"\"\n",
    "sPlotName= 'CONUS404-WBGT-anMax_var-change.jpg'\n",
    "if os.path.isdir(sPlotFile) != 1:\n",
    "    subprocess.call([\"mkdir\",\"-p\",sPlotFile])\n",
    "print( '        Plot map to: '+sPlotFile+sPlotName)\n",
    "fig.savefig(sPlotFile+sPlotName, dpi=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3db83-813e-458b-8d36-618a4a531812",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.5,11.5,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545a427-0e48-40b3-a4fb-f5753a2e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.median(month_max_pgw[targ_years,:], axis=0)-np.median(month_max[targ_years_ref,:], axis=0), cmap=\"coolwarm\",vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010c7a-284b-4460-803d-5d870203c8ba",
   "metadata": {},
   "source": [
    "### Plot change attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c530e9b-62e0-4b4d-9678-1309f60377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_var_categories = ['T2','Q2','PSFC','UV10', 'LW', 'SW']\n",
    "change_cat_colors = ['#e31a1c','#1f78b4','#969696','#33a02c','#a6cee3','#fb9a99']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55316fe-3bfc-4fdc-a429-39a5c5588aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cy=0\n",
    "np.nanmedian(pgw_original_data[:,cy,:]) - np.nanmedian(ctr_original_data[:,cy,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef3906-3110-431f-9822-58c0411afda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "# Histogram plots\n",
    "gs1 = gridspec.GridSpec(2,2) #rgiSize[2])\n",
    "gs1.update(left=0.06, right=0.97,\n",
    "           bottom=0.13, top=0.97,\n",
    "           wspace=0.20, hspace=0.40)\n",
    "\n",
    "for cy in range(len(cities)):\n",
    "    ax = plt.subplot(gs1[0,0])\n",
    "    \n",
    "    mean_change = np.nanmedian(pgw_original_data[:,cy,:]) - np.nanmedian(ctr_original_data[:,cy,:])\n",
    "    # ax.bar('$\\Delta$GWBT$_{max}$',mean_change, align='center', color='k', width=0.1)\n",
    "    \n",
    "    plt.plot([0.1+cy,0.1+cy],[0,mean_change], c='k', lw=1)\n",
    "    plt.plot([0.9+cy,0.9+cy],[0,mean_change], c='k', lw=1)\n",
    "    plt.plot([0.1+cy,0.9+cy],[mean_change,mean_change], c='k', lw=3)\n",
    "    \n",
    "    # Add the components from different variables\n",
    "    orig_change = np.nanmedian(pgw_original_data[:,cy,:]) - np.nanmedian(ctr_original_data[:,cy,:])\n",
    "    mean_change_vars = (np.nanmedian(change_sensitivities[:,cy,:,:], axis=(0,1)) - np.nanmedian(ctr_original_data[:,cy,:]))\n",
    "    change_deltas = orig_change - (np.nanmedian(change_sensitivities[:,cy,:,:], axis=(0,1)) - np.nanmedian(ctr_original_data[:,cy,:]))\n",
    "    \n",
    "    for va in range(change_sensitivities.shape[3]):\n",
    "        if va == 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = np.sum(change_deltas[:va])\n",
    "        plt.arrow(0.3+0.1*va + cy, start, 0, np.sum(change_deltas[:va+1])-start, length_includes_head=True,\n",
    "                  head_width=0.1, head_length=0.05,\n",
    "                  color=change_cat_colors[va],\n",
    "                  lw=3)\n",
    "    \n",
    "    plt.xlim(-0.1, len(cities))\n",
    "    plt.ylim(0, 2.5)\n",
    "\n",
    "for va in range(change_sensitivities.shape[3]):\n",
    "    plt.plot([],[], label = change_var_categories[va], lw = 3, color = change_cat_colors[va])\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xticks(np.array(range(len(cities)))+0.5)\n",
    "ax.set_xticklabels(cities, rotation=90)\n",
    "\n",
    "plt.grid(which='major', color='k', linestyle=':', axis='y', alpha = 0.5)\n",
    "plt.grid(which='minor', color='k', linestyle=':', axis='y', alpha = 0.5)\n",
    "\n",
    "plt.ylabel('annual max. WBGT change [K]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cfcb2-53c6-4346-9114-226b9f688c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cy=0\n",
    "np.nanmean(pgw_original_data[:,cy,:]) - np.nanmean(ctr_original_data[:,cy,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459a925-5a6b-4f48-a58a-e5b778e5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(pgw_original_data[:,0,:]) - np.mean(ctr_original_data[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818c7d4-33dd-4137-aa38-2d33c9cc3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_sensitivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3482b-be62-4f99-9545-ce1a48ce1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_sensitivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff7b95-63b6-416c-a37b-7770af9fd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_original_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f860fc-c3dd-4b19-bcda-3331063dace9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ffd29-0d01-45b5-82af-e92fb7cb74b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914c51c-723f-4980-b0f1-6272af0713c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72933c-d0f3-4e76-bf5e-d6f56e6a8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45468e1e-9756-4e75-8cad-d05c6b8a4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6990be6-fa7f-44bd-b7ab-674f75aa70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_data_sel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d29d77-ac1a-410c-9ec9-b377e4c17f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5778319-809a-4b8d-88a0-cadf4c5553e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de890fae-7165-4576-a1e6-593fcd575c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fa7ab-45ba-499c-a60f-36851a78c22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89df25-85a1-4fe3-af4d-38c2a171d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = 0\n",
    "cy = 0\n",
    "ce = 4\n",
    "\n",
    "plt.plot(change_sensitivities[:,cy,ce,va])\n",
    "plt.plot(pgw_original_data[:,cy,ce], c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e137f74-7ef5-4c03-bc50-36ae8282298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pgw_original_data[:,cy,:], c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2037fdb-7f09-408b-90c5-9ee3095b49e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704abbdc-38d7-4bfe-a111-8bf483ab3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b41f2-494f-4d73-ab03-5d639fa00319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgw_data[yy,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c5faa-9025-4ec6-b1b0-5a902a83d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgw_data[yy,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c455a0-ab94-42a6-a21c-1d70350df9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51bbf4-17ff-4a7a-a8dc-8135ed3af89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(huss[:1,500:501,600:601]),\n",
    "                np.array(tas[:1,500:501,600:601]),\n",
    "                np.array(ps[:1,500:501,600:601]),\n",
    "                np.array(uas[:1,500:501,600:601]),\n",
    "                np.array(vas[:1,500:501,600:601]),\n",
    "                np.array(rlds[:1,500:501,600:601]),\n",
    "                np.array(rsds[:1,500:501,600:601]),\n",
    "                np.array(rlus[:1,500:501,600:601]),\n",
    "                np.array(rsus[:1,500:501,600:601]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58f51b-c1f7-4ee5-b85c-b6e112f2651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e05d2-c55c-4169-aeff-7188827021f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "va=4\n",
    "plt.plot(years_ctr,ctr_data[:,va])\n",
    "plt.plot(years_pgw+42, pgw_data[:,va])\n",
    "plt.plot(years_pgw+42, pgw_data_noTchange[:,va], c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8963988-f21a-4784-9289-cf22e9fc42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a19f2-d0a6-4ed6-98d0-99646b26694f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0df09-0fd7-4e08-875d-cb563c9783c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c510651-c7bf-4d35-a201-5b85f9d559c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff16fd-de09-4a04-a720-ac5dd9b4061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "yy = np.nanmean(ymax_ctr[:,city_loc[cy][1][0]:city_loc[cy][1][1], \\\n",
    "                 city_loc[cy][0][0]:city_loc[cy][0][1],0][:,city_gc], axis=1)\n",
    "\n",
    "xx = np.nanmean(ymax_ctr[:,city_loc[cy][1][0]:city_loc[cy][1][1], \\\n",
    "                 city_loc[cy][0][0]:city_loc[cy][0][1],1:][:,city_gc], axis=1).T\n",
    "\n",
    "# # center y values to reduce high cond. no.\n",
    "# xx = xx - np.average(xx, axis=1)[:, None]\n",
    "\n",
    "def reg_m(y, x):\n",
    "    x = x[::-1]\n",
    "    ones = np.ones(len(x[0]))\n",
    "    X = sm.add_constant(np.column_stack((x[0], ones)))\n",
    "    for ele in x[1:]:\n",
    "        X = sm.add_constant(np.column_stack((ele, X)))\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e21bd-70c6-4252-a08d-d9fb58c36c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_m(yy, xx).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d67b5-f442-4348-98c3-70ce82a11e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xx[1,:], yy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee1956-f004-4ac5-9496-03603f7adc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce901c-3b47-442a-a17a-c284f7f3d1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
